const { getSupabaseClient } = require('./_supabase');
const { spawn } = require('child_process');
const path = require('path');

const TABLE = process.env.EVENTS_TABLE || 'Event List';
const CONFLICT_COL = process.env.EVENTS_ON_CONFLICT || 'event_name_and_link';

// Table schema reference (create this in Supabase):
// create table if not exists events (
//   id bigint generated by default as identity primary key,
//   event_name text not null,
//   event_date text,
//   event_time text,
//   event_location text,
//   event_description text,
//   hosted_by text,
//   price text,
//   event_url text unique,
//   created_at timestamp with time zone default now()
// );

function withCompositeKey(events) {
  return (events || []).map((e) => ({
    ...e,
    // Create a stable unique composite key if column exists in table
    [CONFLICT_COL]: `${(e.event_name || '').trim()} | ${(e.event_url || '').trim()}`,
  }));
}

async function upsertEvents(supabase, events) {
  if (!Array.isArray(events) || events.length === 0) return { count: 0 };
  const rows = withCompositeKey(events);
  // Upsert on unique `event_url`
  const { data, error } = await supabase
    .from(TABLE)
    .upsert(rows, { onConflict: CONFLICT_COL })
    .select('event_url');
  if (error) throw error;
  return { count: data?.length || 0 };
}

async function fetchEvents(supabase, { q, limit = 100 }) {
  let query = supabase.from(TABLE).select('*').order('created_at', { ascending: false });
  if (q) {
    const like = `%${q}%`;
    query = query.or(
      [
        `event_name.ilike.${like}`,
        `event_description.ilike.${like}`,
        `event_location.ilike.${like}`,
        `hosted_by.ilike.${like}`
      ].join(',')
    );
  }
  if (limit) query = query.limit(Number(limit));
  const { data, error } = await query;
  if (error) throw error;
  return data || [];
}

function runPythonScraperAsJson() {
  return new Promise((resolve, reject) => {
    // Try common locations based on current structure:
    // - app/scrape_tech_week_sf.py (not present by default)
    // - scraper/scrape_tech_week_sf.py (one level up from app/)
    // - repo root scrape_tech_week_sf.py (legacy)
    const fs = require('fs');
    const candidates = [
      path.resolve(__dirname, '..', 'scrape_tech_week_sf.py'),
      path.resolve(__dirname, '..', '..', 'scraper', 'scrape_tech_week_sf.py'),
      path.resolve(__dirname, '..', '..', 'scrape_tech_week_sf.py')
    ];
    const scraperPath = candidates.find(p => fs.existsSync(p));
    if (!scraperPath) {
      return reject(new Error('Scraper not found. Expected at app/scrape_tech_week_sf.py or ../scraper/scrape_tech_week_sf.py'));
    }
    const cwd = path.resolve(__dirname, '..');
    const proc = spawn('python3', [scraperPath, '--json'], { cwd });

    let stdout = '';
    let stderr = '';
    proc.stdout.on('data', (chunk) => (stdout += chunk.toString()));
    proc.stderr.on('data', (chunk) => (stderr += chunk.toString()));
    proc.on('error', (err) => reject(err));
    proc.on('close', (code) => {
      if (code !== 0) {
        return reject(new Error(`Scraper exited with code ${code}: ${stderr}`));
      }
      try {
        // The scraper prints progress + JSON; try to extract JSON from the last line if needed
        const trimmed = stdout.trim();
        const maybeJson = trimmed.startsWith('[') ? trimmed : trimmed.split('\n').find((l) => l.trim().startsWith('[')) || trimmed;
        const events = JSON.parse(maybeJson);
        resolve(events);
      } catch (e) {
        reject(new Error(`Failed to parse scraper output as JSON: ${e.message}\n--- stdout ---\n${stdout}\n--- stderr ---\n${stderr}`));
      }
    });
  });
}

// Vercel-style default export handler
module.exports = async (req, res) => {
  const supabase = getSupabaseClient();

  try {
    if (req.method === 'GET') {
      const q = req.query.q || '';
      const limit = req.query.limit || 100;
      const data = await fetchEvents(supabase, { q, limit });
      return res.status(200).json({ ok: true, count: data.length, events: data });
    }

    if (req.method === 'POST') {
      // Optional: allow `mode=refresh` to force re-scrape
      const mode = (req.query.mode || req.body?.mode || 'refresh').toString();
      if (mode !== 'refresh') {
        return res.status(400).json({ ok: false, error: 'Unsupported mode' });
      }

      // Run the local Python scraper and upsert results to Supabase
      const events = await runPythonScraperAsJson();
      const { count } = await upsertEvents(supabase, events);
      return res.status(200).json({ ok: true, upserted: count });
    }

    res.setHeader('Allow', 'GET, POST');
    return res.status(405).json({ ok: false, error: 'Method Not Allowed' });
  } catch (err) {
    console.error(err);
    return res.status(500).json({ ok: false, error: err.message });
  }
};
